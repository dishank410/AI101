# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14SC7oH3XXA3JTwQmyr027KgzcAaGixQO
"""

import numpy as np
import matplotlib.pyplot as plt
from cvxopt import matrix, solvers

# Generate linearly separable data (you can replace this with custom data)
def generate_data():
    np.random.seed(1)
    X1 = np.random.randn(20, 2) - [2, 2]  # Class 1
    X2 = np.random.randn(20, 2) + [2, 2]  # Class -1
    X = np.vstack((X1, X2))
    y = np.hstack((-1 * np.ones(len(X1)), 1 * np.ones(len(X2))))
    return X, y

# Build the Gram matrix for SVM optimization
def compute_gram_matrix(X, y):
    m, n = X.shape
    K = np.zeros((m, m))
    for i in range(m):
        for j in range(m):
            K[i, j] = y[i] * y[j] * np.dot(X[i], X[j])
    return K

# Solve the dual problem of Soft-margin SVM
def solve_svm_dual(X, y, C):
    m = len(y)
    K = compute_gram_matrix(X, y)

    # Define the quadratic optimization problem
    P = matrix(K, tc='d')
    q = matrix(-np.ones((m, 1)), tc='d')
    G = matrix(np.vstack((-np.eye(m), np.eye(m))), tc='d')
    h = matrix(np.hstack((np.zeros(m), C * np.ones(m))), tc='d')
    A = matrix(y.reshape(1, -1), tc='d')
    b = matrix(0.0, tc='d')

    # Solve the QP problem
    solvers.options['show_progress'] = False
    sol = solvers.qp(P, q, G, h, A, b)
    alpha = np.ravel(sol['x'])
    return alpha

# Extract SVM parameters: support vectors, weights, and bias
def compute_svm_params(X, y, alpha, C):
    support_vector_indices = (alpha > 1e-5) & (alpha < C)
    sv_X = X[support_vector_indices]
    sv_y = y[support_vector_indices]
    sv_alpha = alpha[support_vector_indices]
    w = np.sum(sv_alpha[:, None] * sv_y[:, None] * sv_X, axis=0)
    b = np.mean(sv_y - np.dot(sv_X, w))
    return w, b, sv_X

# Plot the data, decision boundary, and margins
def plot_svm(X, y, w, b, support_vectors):
    plt.figure(figsize=(8, 6))

    # Plot data points
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', alpha=0.7, edgecolors='k')
    plt.scatter(support_vectors[:, 0], support_vectors[:, 1],
                s=100, facecolors='none', edgecolors='k', label='Support Vectors')

    # Decision boundary and margins
    x_min, x_max = -5, 5
    xx = np.linspace(x_min, x_max, 50)
    yy = (-w[0] * xx - b) / w[1]
    margin = 1 / np.linalg.norm(w)
    yy_down = yy - margin
    yy_up = yy + margin

    plt.plot(xx, yy, 'k-', label="Decision boundary")
    plt.plot(xx, yy_down, 'k--', label="Margin")
    plt.plot(xx, yy_up, 'k--')

    plt.xlim(x_min, x_max)
    plt.ylim(x_min, x_max)
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.title("Linear Soft-Margin SVM")
    plt.legend()
    plt.show()


X, y = generate_data()
C = 1.0  # Regularization parameter
alpha = solve_svm_dual(X, y, C)
w, b, support_vectors = compute_svm_params(X, y, alpha, C)
plot_svm(X, y, w, b, support_vectors)

'''Didn't work properly though ðŸ¥²'''